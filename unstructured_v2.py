# -*- coding: utf-8 -*-
"""Unstructured_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D7F86j8UZuxok0PspnbavAW-74KZRCe4
"""

# Use Unstructured to parse the PDF. While unstructured has a core library for processing PDFs, HTML etc, they use
# another library, unstructured_inference, for PDFs with Tables.
# Unstructured_inference uses the YOLOX ML model to parse the PDF. PDF libraries like PDFMiner, Camelot etc are not used.
# However, YOLOX has challenges with complex Tables. Specifically, we seem to get many unnecessary "UncategorizedText" elements.
# Almost like a Table got blown up.
# This code REPLACES YOLO's Tables with Tables from Tabula (in a DataFrame Format) and also eliminates the spurious
# UncategorizedTexts.
# So it can be considered as a hybrid approach using a ML Model & conventional coding.
# While "Chunking By Title" comes out-of-the-box with unstructured-core, there are some challenges in converting objects from
# unstructured_inference to the unstructured-core model, hence using a Custom Chunk By Title approach.
# Trying to get help from unstructured community on the above.
# ToDo - Merge Tables spanning multiple Pages

#SETUP
# !pip install "unstructured[pdf]"
# !apt-get install -y poppler-utils
# !pip install chromadb
# !pip install tabula-py
# #!pip install "camelot-py[cv]" -q
# !pip install 'PyPDF2<3.0'
# #!apt-get install ghostscript
# !pip install unstructured-inference
# #!pip install paddepaddle-gpu
# !pip install "unstructured.PaddleOCR"
import os

os.environ['TABLE_OCR'] = 'paddle'
# ! apt install tesseract-ocr

import unstructured
from unstructured.partition.pdf import partition_pdf
import tabula
import pandas as pd
import PyPDF2
from unstructured_inference.models.base import get_model
from unstructured_inference.inference.layout import DocumentLayout

#Config
pdf_path = "/content/texas-sla.pdf"
MAX_CHARACTERS_PER_CHUNK = 2500
MODEL_NAME = "yolox"

def get_number_of_pages(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfFileReader(file)
        return reader.numPages

#Use Tabula to infer Tables as Dataframes, Indexed By Page.
def get_all_tables_by_page(pdf_path):
    # Get the total number of pages in the PDF
    total_pages = get_number_of_pages(pdf_path)

    # Dictionary to store tables with their page numbers
    tables_as_dataframes_by_page = {}

    for page in range(1, total_pages + 1):
        dataframes = tabula.read_pdf(pdf_path, pages=page)
        # If tables are found on the page, store them in the dictionary
        if dataframes:
            tables_as_dataframes_by_page[page] = dataframes
    return tables_as_dataframes_by_page

model = get_model(MODEL_NAME)
tables_as_dataframes_by_page = get_all_tables_by_page(pdf_path)
#Invoke Inference
layout = DocumentLayout.from_file(pdf_path, detection_model=model)

#Helper function to merge smaller Chunks
def merge_small_chunks_v3(chunks, min_chunk_length):
    # Helper function to get the text length of an element
    def get_length(element):
        length = len(element.text)
        return length

    merged_chunks = []
    temp_chunk = []
    temp_length = 0

    for chunk in chunks:
        chunk_length = sum(get_length(el) for el in chunk)
        if temp_length + chunk_length < min_chunk_length:
            temp_chunk.extend(chunk)
            temp_length += chunk_length
        else:
            if temp_chunk:
                merged_chunks.append(temp_chunk)
                temp_chunk = []
                temp_length = 0
            merged_chunks.append(chunk)

    # Append any remaining temp_chunk
    if temp_chunk:
        merged_chunks.append(temp_chunk)

    return merged_chunks

#Iterate through Elements of the PDF and Chunk/Group by Section-Header
def chunk_by_section_headers(layout):
    element_to_page_map = {}
    chunks = []
    current_chunk = []
    table_tracker = set()
    all_elements = []
    pages = len( layout.pages )
    #Unstructured uses a 0 based index, unlike Tabula.
    for i in range(pages):
        index_of_table_within_page = 0
        for idx, el in enumerate(layout.pages[i].elements):
            #Ignore Images
            if el.type in ["Image", "UncategorizedText"]:
                continue

            #Consider Page-header on only the first page
            if el.type == "Page-header" and i != 0:
                continue

            if el.type == 'Section-header':
                chunks.append(current_chunk)
                current_chunk = []
                current_chunk.append(el)
            elif el.type == "Table":
                el.text = tables_as_dataframes_by_page[i+1][index_of_table_within_page].to_string(index=False)
                index_of_table_within_page += 1
                current_chunk.append(el)
            else:
                current_chunk.append(el)

            #Page Number Metadata is needed to show the PDF Page in UI
            element_to_page_map[el.text] = i+1
    chunks.append(current_chunk)

    merged_chunks_list = merge_small_chunks_v3(chunks, MAX_CHARACTERS_PER_CHUNK)
    return merged_chunks_list, element_to_page_map

def print_chunk(chunks_param):
    length = 0
    for el in chunks_param:
        length += len(el.text)

    for el in chunks_param:
#        if type(el) == "unstructured_inference.inference.layoutelement.LayoutElement":
        print( el.text)
    print("------------------------------------------------------------------")

#Get Chunks by Section-Headers and store them in a VectorDB/ElasticSearch
chunks_list, element_to_page_map = chunk_by_section_headers(layout)
for i, new_chunk in enumerate(chunks_list):
    print_chunk(new_chunk)